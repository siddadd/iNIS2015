
Due to the capacity of human vision systems for highly complex processing at very low power, many brain-inspired algorithms and architectures have been proposed to emulate the human visual cortex.~\cite{Nere2011,Chen2014,Kestur2012}. %[YiranChen UPitt, Qingriu Syracuse, NEC CNN]. 

%DSPs and Heterogenous Architectures
Digital signal processors (DSPs) have been a universally accepted alternative to general purpose CPUs for seamless multimedia processing. 
The Qualcomm Hexagon DSP instruction set architecture (ISA) contains numerous special-purpose instructions designed to accelerate key multimedia kernels such as 
sliding window filters~\cite{hexagon}. 
Heterogeniety has often been considered as a viable solution to handle the increasingly varying nature of workloads that need to be run today~\cite{micro2010}.
Texas Instruments has a heterogenous multi-core DSP targeted for real-time vision applications based on their Keystone architecture. 
To tackle the diverse field of vision many other heterogenous architectures have been proposed that take advantage of 
customized flows for regular systolic operations while
using the traditional von Neumann architecture for handling control logic and other irregular data operations. A heterogenous server architecture consisting of 
many small cores for low power and high throughput coupled with custom hardware accelerators was designed in ~\cite{Iyer2011}.
In ~\cite{HPCA2015}, the authors explored architectural heterogeniety by using customized data-flows for many vision-based applications targeted at retail, 
security, etc.

%Custom Accelerators
Most vision applications demand high throughput and 
specialized accelerators have shown to be extremely performance-friendly for computationaly intensive tasks such as 
face detecion ~\cite{violafccm}, pedestrian detection~\cite{sips2014}, object recognition~\cite{Maashri2012a} and object detection~\cite{Bae2011}. In ~\cite{micro2008}, the authors propose a benchmarking suite - VISBench (Visual, Interactive, Simulation Benchmarks) - and find that a MIMD rather than a SIMD architecture gives better performance.

%CNNs
Even though Convolutional Neural Networks (CNNs) were explored in the early 1990s for vision applications~\cite{giles1997}, they have resurfaced again after a long hiatus and become extremely popular in the past couple of years. 
This successful comeback can be attributed to two major phenomena:
(1) the existence of large amount of data (needed to train the network well) with the evolution of the digital era, and (2) the development of 
custom hardware (required for acceleration) now being used for CNNs. 

In the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)
conducted in 2012, the winning team trained a CNN consisting of five convolutional and three fully-connected layers. Importantly, the depth of the CNN is critical to 
its recognition capabilities since the authors found that removing any convolutional layer resulted in inferior performance~\cite{NIPS2012}. This CNN would need
more than 80 million operations and over 100,000 data transfers~\cite{XilinxCNN}.

More recent and advanced CNN architectures have 10 to 20 layers of Rectified Linear Units, hundreds of millions of weights, and billions of connections between units.
The reader is pointed to ~\cite{Bengio2009} for insights on deep architectures in general and ~\cite{DNNNature2015} for CNN-based learning and their recent advances. 

From a systems perspective, ~\cite{Farabet2009} mapped an earlier Convolutional Network based face-detection task onto custom hardware. More recently, ~\cite{Chen2014} recently proposed an architecture for CNNs and Deep 
Neural Networks (DNNs) that minimized memory transfers thus achieving high
throughput with small area, power and energy footprint. ~\cite{DaDianNao} furthered this by proposing a training and inference accelerator 
capable of providing GPU-like bandwidth in ASIC-like power budgets.
